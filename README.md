Much of the code I wrote based on the guide from github user yas-sim, who broke down the classic example of building 
an LLM from scratch with LLaMA architecture (which has been done many times before)
into a more digestible form. I attach his repo below for you to see.

https://github.com/yas-sim/micro-llama/blob/1740d2064331c25d183d68714b67c4671772454d/README.md 

Note on text: The Christine de pizan.txt has an issue in which the translator's notes (in all CAPS) were left in and some words were not properly imported
leaving them adjoined to other words or with characters missing. I did not have time to completely clean the data although I tried my best with a simple 
script and by manual clean up in some areas.

